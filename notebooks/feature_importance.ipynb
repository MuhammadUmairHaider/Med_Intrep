{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis (Integrated Gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.dataset import load_data\n",
    "from src.interpretability.feature_importance.integrated_gradients import LLMIsDefault\n",
    "from src.interpretability.viz.utils import plot_token_importance, plot_text_heatmap\n",
    "from src.model_utils import get_latest_checkpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found latest checkpoint: ../checkpoints/final_medgemma_model\n",
      "Loading tokenizer from base model: google/medgemma-1.5-4b-it...\n",
      "Loading model: ../checkpoints/final_medgemma_model...\n",
      "Detected PEFT adapter at ../checkpoints/final_medgemma_model. Loading base model google/medgemma-1.5-4b-it first...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f55cf6a22747d1a50d35a24b2a7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with gradient checkpointing enabled.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "# Check for checkpoints in the parent directory\n",
    "CHECKPOINT_DIR = \"../checkpoints\"\n",
    "\n",
    "model_id = get_latest_checkpoint(DEFAULT_MODEL_ID, checkpoint_dir=CHECKPOINT_DIR)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading tokenizer from base model: {DEFAULT_MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL_ID)\n",
    "\n",
    "print(f\"Loading model: {model_id}...\")\n",
    "\n",
    "# Check if model_id is a PEFT adapter (contains adapter_config.json)\n",
    "is_adapter = False\n",
    "if os.path.isdir(model_id) and \"adapter_config.json\" in os.listdir(model_id):\n",
    "    is_adapter = True\n",
    "\n",
    "if is_adapter:\n",
    "    from peft import PeftModel\n",
    "    print(f\"Detected PEFT adapter at {model_id}. Loading base model {DEFAULT_MODEL_ID} first...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        DEFAULT_MODEL_ID, \n",
    "        dtype=torch.bfloat16, \n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_id)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        dtype=torch.bfloat16, \n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "\n",
    "# Enable Gradient Checkpointing to save memory during IG\n",
    "model.gradient_checkpointing_enable()\n",
    "model.eval()\n",
    "print(\"Model loaded with gradient checkpointing enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1479 test samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>study_name</th>\n",
       "      <th>icd_o_3_site</th>\n",
       "      <th>icd_o_3_histology</th>\n",
       "      <th>icd_o_3_behavior</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>TCGA-CS-5394</td>\n",
       "      <td>LGG</td>\n",
       "      <td>Brain Lower Grade Glioma</td>\n",
       "      <td>C719</td>\n",
       "      <td>9401</td>\n",
       "      <td>3</td>\n",
       "      <td>FINAL DIAGNOSIS: 1. LEFT FRONTAL TUMOR: ANAPLA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>TCGA-B0-4848</td>\n",
       "      <td>KIRC</td>\n",
       "      <td>Kidney renal clear cell carcinoma</td>\n",
       "      <td>C649</td>\n",
       "      <td>8310</td>\n",
       "      <td>3</td>\n",
       "      <td>FINAL DIAGNOSIS. PART 1: LEFT KIDNEY, RADICAL ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient_id cancer_type                          study_name  \\\n",
       "3778  TCGA-CS-5394         LGG           Brain Lower Grade Glioma    \n",
       "2402  TCGA-B0-4848        KIRC  Kidney renal clear cell carcinoma    \n",
       "\n",
       "     icd_o_3_site  icd_o_3_histology  icd_o_3_behavior  \\\n",
       "3778         C719               9401                 3   \n",
       "2402         C649               8310                 3   \n",
       "\n",
       "                                                   text  \n",
       "3778  FINAL DIAGNOSIS: 1. LEFT FRONTAL TUMOR: ANAPLA...  \n",
       "2402  FINAL DIAGNOSIS. PART 1: LEFT KIDNEY, RADICAL ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, test_df = load_data('../tcga_reports_valid.csv')\n",
    "print(f\"Loaded {len(test_df)} test samples.\")\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "sample = test_df.sample(1).iloc[0]\n",
    "text = sample['text']\n",
    "print(f\"Patient: {sample['patient_id']} | Cancer: {sample['cancer_type']}\")\n",
    "\n",
    "prompt = f\"### Instruction:\\nAnalyze the report.\\n\\n### Input:\\n{text}\\n\\n### Response:\\n\"\n",
    "\n",
    "ig = LLMIsDefault(model, tokenizer)\n",
    "\n",
    "# Reduced internal_batch_size to 1 to avoid OOM\n",
    "res = ig.interpret(prompt, n_steps=20, internal_batch_size=1)\n",
    "\n",
    "plot_token_importance(res['tokens'][-20:], res['scores'][-20:], title=f\"Importance for '{res['target_token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def display_heatmap(tokens, scores):\n",
    "    html = plot_text_heatmap(tokens, scores)\n",
    "    display(HTML(html))\n",
    "\n",
    "display_heatmap(res['tokens'], res['scores'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
